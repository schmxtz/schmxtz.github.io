<!DOCTYPE html>
<html>

<head>
    <title>JPTranscription</title>
    <link rel="icon" type="image/x-icon" href="/images/favicon.ico">
    <link rel="stylesheet" href="style.css">
    <script type="text/javascript" src="index.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
    <div class="header">
        <div class="links-container">
            <a href="index.html">home</a>
            <span class="separator">|</span>
            <a href="https://github.com/schmxtz">github</a>
            <span class="separator">|</span>
            <a href="jptranscription.html">jptranscription</a>
            <span class="separator">|</span>
            <a href="aboutme.html">about me</a>
            <span class="separator">|</span>
            <button onclick="invertColor()"><i class="fa fa-adjust fa-2x" aria-hidden="true"></i></button>
        </div>
    </div>
    <hr>
    <h2>Introduction</h2>
   The idea behind this project started when I was learning Japanese. I realized that the pronunciation of Japanese is very similar to German in some ways. Or
   at least you can "recreate" German words using Japanese Katakana.<br>
   
   My first thought then, was to split German words into their syllables and then have a mapping
   to transcribe each mapping to its Japanese counterpart. However according to <a href="https://german.stackexchange.com/questions/70223/how-many-different-syllables-does-the-german-language-have">this</a> post,
   there are around 140000 unique syllables in German - there has to be a better way.<br>
   So then thinking like a mathematician, I first <a href="https://en.wikipedia.org/wiki/Reduction_(mathematics)">reduced</a> the problem to a smaller subset. The way I achieved this, was to first get the <a href="https://en.wikipedia.org/wiki/International_Phonetic_Alphabet">phonetic representation</a> of
   each word and then create a mapping for phonetic blocks to Katakana blocks. This surprisingly worked as there are only around 600 entries in my mapping so far.
    <hr>
    <h2>Implementation</h2>
    The first issue I ran into was, how do I get the phonetic representation of all German words. And is it possible to find something that is free to use and open-source?<br>
    My first idea was to use the python library <a href="https://github.com/rhasspy/gruut-ipa">gruut-ipa</a>, but I ran into some issues very early on. Some of
    the transcriptions were not correct. I wanted to try to avoid that as much as possible, so I kept looking. Eventually I found out that <a href="https://www.wiktionary.org/">Wikitionary</a> has a 
    broad database for German words including their phonetic representation and is licensed under the <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC license</a>.<br>
    From <a href="https://kaikki.org/dictionary/rawdata.html">kaikki.org</a> (thank you very much for hosting a compressed extract of each language), I was able to download the
    list of all words. I wrote a small script to parse it and only save the relevant data - the word and its phonetic representation - and decreased the file 
    down to ~70MB. This is also very useful since it is small enough to represent the data as an in-memory dictionary. Thus I can avoid having to setup some sort of
    database and is much faster for word lookups. <br>
    Now I started making the mapping for phonetic chunks to their Katakana equivalent. I created an algorithm which greedily tries to match the longest possible
    phonetic chunk. The phonetic representation for the word "danke" is "ˈdaŋkə". The first character can be ignored as it just tells the reader to put put a stress
    on the start. Then the next longest chunks are "da", "ŋ" and "kə", which maps to "ダ" (da), "ン" (n) and "ケ" (ke). The design was inspired by the site <a href="https://jisho.org/search/%E3%83%95%E3%83%BC%E3%83%90%E3%83%BC%E5%AE%B6%E3%81%AF%E3%82%A6%E3%82%A3%E3%83%BC%E3%83%B3%E3%81%AE%E8%BF%91%E3%81%8F%E3%81%AB%E4%BD%8F%E3%82%93%E3%81%A7%E3%81%84%E3%81%BE%E3%81%99%E3%80%82">jisho.org</a>, with placing the
    way how to read the word, above the word itself.

    <hr>
    <h2>External dependencies</h2>
    <h3><a href="https://github.com/schmxtz/jptranscription">jptranscription</a></h3>
    In the main core logic part of my application I only have one external dependency (two if you include the word dictionary list) which is a <a href="https://pypi.org/project/split-words/">library</a> to split compound-nouns into their respective "base nouns". This is needed because
    Germans love to put many nouns together to create new words. Usually those are not in the dictionary which is why I need to split them into their original parts. I will implement this
    myself in the future.
    <h3><a href="https://github.com/schmxtz/jptranscriptionapi">jptranscriptionapi</a></h3>
    For the API I'm using fastapi because it's fast and easy to deploy. Obviously the library above is one dependency. I'm also using <a href="https://pypi.org/project/spacy/">spacy</a>, which is a NLP libary that performs
    the word splitting for me. This is not a trivial problem for German due to its punctuation rules and other weird grammar. I'm also using <a href="https://pypi.org/project/num2words/">num2words</a> to transcribe decimal numbers
    to their written equivalent. And lastly I'm using <a href="https://pypi.org/project/deep-translator/">deep-translator</a>, which stems the API translation calls for me.
    <h3><a href="https://github.com/schmxtz/schmxtz.github.io">schmxtz.github.io</a></h3>
    The website itself is written entirely in plain HTML, javascript and css. I like simplicity and performance, so I avoided using any web frameworks. I'm working with Angular, Ionic and Capacitor for my work,
    so I wasn't very interested in using it here too.
    <br><br><br><br>
</body>

</html>